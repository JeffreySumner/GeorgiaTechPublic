>> Hi, I'm Joel Sokol,
director of the Master of Science in Analytics degree at
Georgia Tech and a professor in
Georgia Tech's Stewart School of ISWE.
In previous lessons, we talked about how to
measure the effectiveness of a model.
We saw why and how to separate our data into three sets,
training, validation, and test.
In this lesson, we will see one last way to
use the data cross-validation.
Let's look again at our flowchart of how to use our data.
There's something in here that often gets people nervous.
What if there are a couple of important data points that
only show up in the validation or test sets,
then that data isn't part of the training set,
so the model doesn't know about it and that means
the model doesn't fit itself to
these important pieces of data.
Cross-validation is a way to avoid that problem.
There are several types of cross-validation.
In this lesson, we'll talk about k-fold cross-validation.
I guess k seems to be
a popular letter to use in analytics.
We have k-means, k nearest neighbor,
and now k-fold cross-validation.
K refers to something different in each of them,
but in each case, it shows
how many of something we have.
Let's see, k-fold cross-validation.
Let's zoom into the training and
validation steps in our flowchart.
Suppose we have, say,
20% of our data reserved for testing,
and the other 80% will
be used for training and validation.
In this approach, we'll be doing
training and validation together.
We can split this data up into k parts.
Here, let's say k is four.
Instead of doing the training and validation once,
we can do them multiple times
with slightly different datasets.
For each of our k equals four parts of the dataset,
we can train the model on
three parts and validate it on the fourth.
So we train on a data set of parts 1,
2, and 3 and evaluate the model using part 4.
Then we train on the combination of parts 1, 2,
and 4 and evaluate the model using part 3.
Then we train on the combination of parts 1,
3, and 4 and evaluate using part 2.
And then we train on the combination of parts 2,
3, and 4 and evaluate using part 1.
Every data point has been used to train three of
the models so we don't have to
worry that important data was left out.
When we're comparing models
to see which we should choose,
we should just take the average of all four evaluations.
More generally, if we're splitting the training and
validation data into k parts,
then for each of the k parts,
we train the model on
all the other parts and
evaluate it on the one remaining part.
Then we averaged the k evaluations
to estimate the model's quality.
There's no standard number to use as k,
although k equals 10 has become fairly common.
There's just one last thing we need to think about.
Once we've used cross-validation to choose a model,
what exactly is the model we've chosen?
Let's think about it. Suppose we used
four-fold cross-validation on
a support vector machine model for classification.
For each of the four ways to split the data,
we get a slightly different classifier.
So which one do we use?
The answer is none of them.
And we shouldn't average the coefficients across
the four splits either because
there's no guarantee that will work well.
Instead, once we have the model selected,
we can train it again using
all four parts of the data together.
So cross-validation helps you
make better use of your data.
And the averaging process helps
give a better estimate of model quality.
Combined that helps us choose a model more effectively so
it's no surprise that it has become
a commonly used approach in analytics.